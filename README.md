# ğŸ“˜ DeepSeek-OCR LoRA Fine-tuning (Chinese OCR)

A complete, reproducible pipeline for fine-tuning DeepSeek-OCR-3B using LoRA, Unsloth, and HuggingFace.
This project focuses on improving real-world Chinese OCR accuracy (CER), and provides a fully working multi-modal training & evaluation framework.

## ğŸ”¥ 1. Motivation

DeepSeek-OCR delivers strong zero-shot OCR, but in many business scenarios (bills, receipts, medical records, screenshots) baseline accuracy is unstable:

Frequent hallucination ("è¯·è¾“å…¥...", "å›¾ç‰‡å†…å®¹æ˜¯...")

Character order errors

Over-generated text

Shape-similar character confusion (â€œç˜¦/å—â€, â€œç‚¹/å…¸â€)

High baseline CER (â‰ˆ1.0)

Goalï¼šBuild a LoRA fine-tuning pipeline that can significantly improve OCR performance on domain dataâ€”
and make the whole process reproducible, interrupt-resistant, and suitable for long training sessions.

## ğŸš€ 2. Project Highlights
âœ” Full multi-modal training pipeline (image + text)

DeepSeek-OCR requires custom fields such as:

images

images_seq_mask

images_spatial_crop

This repo includes a complete DataCollator implementation that correctly builds these tensors.

âœ” LoRA on language head only

Efficient LoRA config:

target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
r = 16
lora_alpha = 16


This reduces memory while improving text decoding quality.

âœ” 5 ä¸‡è®­ç»ƒæ•°æ® + 2000 éªŒè¯æ•°æ®

Using priyank-m/chinese_text_recognition (HF public dataset), auto-converted into DeepSeek-OCR multi-modal format.

âœ” Robust checkpointing for long training

Training runs for hours â†’ this repo supports:

save_steps = 500

Resume from latest checkpoint

Safe loading of both base model + LoRA adapter

âœ” Full evaluation suite

Includes:

1) Perplexity (HF evaluate)
2) CER (Character Error Rate)

A custom CER evaluator is provided to measure real OCR accuracy.

## ğŸ“‚ 3. Project Structure
deepseek-ocr-finetune/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ocr_dataset.py              # dataset loader & converter
â”‚
â”œâ”€â”€ collator/
â”‚   â”œâ”€â”€ deepseek_ocr_collator.py    # critical multi-modal DataCollator
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_lora.py               # main training script with checkpointing
â”‚
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ eval_ppl.py                 # evaluation: perplexity
â”‚   â”œâ”€â”€ eval_cer.py                 # evaluation: CER
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

## ğŸ§© 4. Data Format (DeepSeek-OCR Expected Structure)

Each sample is converted into:

{
  "messages": [
    {
      "role": "<|User|>",
      "content": "<image>"
    },
    {
      "role": "<|Assistant|>",
      "content": "è¯†åˆ«åçš„æ–‡æœ¬å†…å®¹"
    }
  ],
  "image": { "bytes": ... }
}

Why this matters

DeepSeek-OCR is multi-modal.
Only this structure correctly aligns:

Visual patches

Language tokens

Ignore mask

Assistant-only training regions

This repo includes a fully working converter.

## ğŸ§  5. Training Pipeline
Train with:
from transformers import TrainingArguments, Trainer
from unsloth import FastVisionModel

### Load model with custom remote code
model, tokenizer = FastVisionModel.from_pretrained(
    "unsloth/DeepSeek-OCR",
    trust_remote_code = True,
    load_in_4bit = False
)

### Apply LoRA
model = FastVisionModel.get_peft_model(
    model,
    target_modules=[...],
    r=16,
    lora_alpha=16
)

TrainingArguments example:
training_args = TrainingArguments(
    output_dir = "./checkpoints",
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 4,
    learning_rate = 1e-4,
    warmup_steps = 200,
    max_steps = 5000,

    save_strategy = "steps",
    save_steps = 500,
    save_total_limit = 3,

    eval_strategy = "steps",
    eval_steps = 500,
    logging_steps = 50,

    fp16 = True,
    remove_unused_columns = False,
)


Run training:

trainer = Trainer(
    model = model,
    tokenizer = tokenizer,
    data_collator = DeepSeekOCRDataCollator(...),
    train_dataset = train_ds,
    eval_dataset = eval_ds,
    args = training_args,


trainer.train()

## ğŸ“ˆ 6. Evaluation Results (Checkpoint: 1000 steps)

Evaluated on 50 / 100 / 200 / 1000 / 2000 samples.

Samples	CER
50	0.6957
100	0.7078
200	0.7006
1000	0.7253
2000	0.7006
Interpretation

Baseline CER â‰ˆ 1.0ï¼ˆå‡ ä¹ä¸å¯ç”¨ï¼‰

Fine-tuned CER â‰ˆ 0.70

å‡å°‘çº¦ 30% å­—ç¬¦çº§é”™è¯¯

æ¨¡å‹çš„â€œè¡¥è¯/ä¹±åºâ€é—®é¢˜æ˜¾è‘—å‡å°‘

é•¿å¥ç»“æ„ä¿æŒæ›´ç¨³å®š

æ›´é€‚åˆä½œä¸ºç»“æ„åŒ–æŠ½å–çš„å‰ç½® OCR æ¨¡å‹

## ğŸ“‰ 7. Example Outputs

Example 1:

GT   : å…šç¬¬ä¸€æ¬¡ä»£è¡¨å¤§ä¼š
Pred : ç¬¬ä¸€æ¬¡ä»£è¡¨å¤§ä¼šå¤§å…š
CER  : 0.571


Example 2:

GT   : æµ·ä¿¡LED46EC3
Pred : æµ·ä¿¡LED46ec3
CER  : 0.09


Example 3:

GT   : ï¼Œä¹Ÿæ²¡æœ‰ç˜¦é«˜
Pred : ä¹Ÿæ²¡æœ‰ç˜¦é«˜çš„
CER  : 0.33


ğŸ‘‰ æ˜æ˜¾å‡å°‘å†—ä½™å­—ã€æ–¹å‘é”™è¯¯å’Œè¡¥è¯ã€‚

## ğŸ›  8. Troubleshooting & Common Issues

This repo includes fixes for:

âœ” transformers remote code loading failure
âœ” DeepseekOCRConfig incompatible with AutoModel
âœ” CPU/GPU mismatch in patch encoder
âœ” dynamic_preprocess undefined
âœ” DataCollator producing empty batches
âœ” Accelerate mixed precision crashes
âœ” HuggingFace removing unused columns (must disable)
âœ” Vision token masks not aligned with labels

Every issue above has been solved and documented inside the repo.

## ğŸ§ª 9. Roadmap
âœ… LoRA å¾®è°ƒï¼ˆå½“å‰ï¼‰
â¬œ æ”¯æŒå…¨å‚æ•°å¾®è°ƒ (DeepSpeed ZeRO-2/3)
â¬œ æ”¯æŒæ¨¡å‹åœ¨ç¥¨æ® OCR / åŒ»ç–— OCR ä¸Šç»§ç»­æ‰©å±•
â¬œ Demo WebUIï¼ˆGradioï¼‰
â¬œ ONNX / TensorRT æ¨ç†åŠ é€Ÿ
â¬œ Releasing real-world evaluation set

# ğŸ”— LoRA æƒé‡è·å–

æœ¬ä»“åº“ä»…åŒ…å«è®­ç»ƒä¸è¯„ä¼°ä»£ç ï¼Œä¸ç›´æ¥æ‰˜ç®¡å¤§æ¨¡å‹æƒé‡ã€‚

- DeepSeek-OCR ä¸­æ–‡åœºæ™¯ LoRA æƒé‡ï¼ˆstep=1000ï¼‰ç›®å‰å­˜æ”¾äºä¸ªäººäº‘ç›˜
